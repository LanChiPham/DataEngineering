1. What is Hadoop MapReduce?
    - For proccesing large datasets in parallel across hadoop cluster, hadoop MapReduce framework is used. 

2. What are the difference between relational database and HDFS?
    - There are 6 major categories we can define RDMBS and HDFS. They are:
    a. Data types
    b. proccesing
    c. Schema on read vs write 
    d. read/write speed 
    e. cost 

    Best fit use case:
    RDMBS:                                                         HDFS
        1. in RDMBS, it relies on structured data.                 1. Any kind of data can be stored into Hadoop. ie: structured, unstructed, semi-structured data.
        2. RDMBS provides limited or no proccesing capabilities    2. Hadoop allows us to process the data which is distributed across the cluster in a parallel fashion.
        3. RDMBS is based Schema on write                          3. Hadoop follows Schema on read policy
        4. In rdmbs, reads are fast because the schema is already  4. Writes are fast in hadoop because no schema validation happens during hdfs write
        known
        5. Licensed softwarem therefore, need to pay for software  5. Hadoop is open source framework, hence no need to pay for software               
        6. RDMBS is used for OLTP (Online transactional            6. Hadoop is used for data discovery, data analytics or OLAP system.
        processing) system.


3. Explain Bigdata and explain 5v's of Bigdata?
    - Bigdata is a term of collection of large and complex datasets, that makes it to difficult proccesing using relational database management tools or traditional data processing applications.
    It is difficult to capture, visualize, curate, search, share, transfer and analyze bigdata.

    IBM has defined bigdata with 5v's they are:
     a. volume 
     b. velocity
     c. variety
     d. veracity
     e. value: it is good to have access to bigdata but unless we turn into value it is useless. which means using bigdata adding benifits to the organizations, and they are seen ROI using bigdata.


4. What is Hadoop and its component?
    - When bigdata emerged as a problem, Hadoop evolved as Solution to bigdata. Apache Hadoop is a framework which provides us various services or tools, to store and process the bigdata.
    - It helps in analyzing bigdata and making business decisions out of it, which cannot be done using traditional systems.
    - Main components in hadoop are:
    a. Storage (Namenode, DataNode)
    b. Processing framework yarn (resrource manager, node manager)


5. What are HDFS and Yarn?
    HDFS (Hadoop distributed file system) is the storage unit of Hadoop. It is responsible for storing different kinds of data as blocks in a distributed environment.
    It follows master and slave topology.
    - NameNode: is the master node in the distributed environment and it maintains the metadata information for the blocks of data stored in HDFS like blocj location, replication factors, etc
    - DataNodes: are the slave nodes, which are responsible for storing data in the HDFS. NameNode manages all the DataNodes.

    - YARN (Yet Another Resource Negotiator) is the processing framework in Hadoop, which manages resources and provides an execution environment to the processes.

    - ResourceManager: It receives the processing requests, and then passes the parts of requests to corresponding NodeManagers accordingly, where the actual processing takes place. It allocates resources to applications based on the needs. 

    - NodeManagers: NodeManager is installed on every DataNode and it is responsible for the execution of the task on every single DataNode. 


6. Tell me about various Hadoop Daemons and their roles in Hadoop cluster?
    - Generally approach this question by first explaning the HDFS daemons.
    ie: NameNode, DataNode, and Secondary NameNode, and then moving on to the Yarn daemons. 
    ie: ResourceManager, and NodeManager, and lastly explaning the JobHistoryServer.
    - JobHistoryServer: it maintains information about MapReduce jobs after the application master terminates.


7. Compare HDFS with Network attached service (NAS)?
    a. Network-attached storage (NAS) is a file-level computer data storage server connected to a computer Network providing data access to a heterogeneous group of clients. 
    NAS can either be a hardware or software which provides services for storing and accessing files. 
    Whereas Hadoop Distributed File System (HDFS) is a distributed filessystem to store data using commodity hardware. 
    In HDFS, Data blocks are distributed across all the machines in a cluster.
    Whereas in NAS data is stored on a dedicated hardware.


8. List the difference between Hadoop 1.0 vs Hadoop 2.0?
    - In Hadoop 1.x, "NameNode" is the single point of failure. In Hadoop 2.x, we have Active and Passive "NameNode". If the active "NameNode" fails, the passive "NameNode" takes charge.
    Because of this, high availability can be archived in Hadoop 2.x

    - In Hadoop 2.x, YARN provides a central resource manager. with YARN, you can now run multiple applications in Hadoop, all sharing a common resource. MRV2 is a particular type of 
    distributed application that runs the MapReduce framework on top of YARN. Other tools can also perform data processing via YARN, which was a problem in Hadoop 1.x

9. What are active and passive NameNode?
    - In a high availability architecture, there are 2 namenodes.
    i.e: 
    a. Active "NameNode" is the "NameNode" which works and runs in the cluster
    b. Passive "NameNode" is a standby "NameNode", which has similar data as active "NameNode"

    When the active "NameNode" fails, the passive "NameNode" replaces the active "NameNode" in the cluster.

10. Why does one remove or add datanodes frequently?
    - Most attractive features of the Hadoop framework is its utilization of commodity hardware. 
    However, this leads to frequent "DataNode" crashes in a Hadoop cluster.
    - Another striking feature of Hadoop framework is the ease of scale in accordance with the rapid growth in data volume. This is why hadoop admins work is to commission or decommnission nodes in hadoop cluster.


11. What happens when two clients tries to access same file in hdfs?
    - When first client requests for file or data hdfs providing access to write, but when second client request it rejects by saying already another client accessing it. 


12. How does nameNode tackles data node failures?
    - NameNode periodically receives a heartbeat(signal) from each of the DataNode in the cluster, which implies DataNode is functioning properly.

13. What will you do when NameNode is down?
    - Use the file system metadata replica (Fslimage) to start a new NameNode.
    - Then, configure the DataNodes and clients so that can acknowledge this name NameNode, that is started.

14. What is checkpoint?
    - Checkpointing is a process that takes an FsImage, edit log and compacts them into a new FsImage.
    - Thus, instead of replaying an edit log, the NameNode can load the final in-memory state directly from FsImage.

15. What is Hdfs fault tolerant?
    - When a data stored in over hdfs, namenode replicates the data in server datanode, which has default value of 3.
    - If any DataNode fails NameNode automatically copies data to another datanode to makesure data is fault tolerant

16. Can Namenode and datanode are commodity hardware?
    - No, because NameNode built in with high memory space, and with higher quality software but datanode built in with cheaper hardware. 

17. Why do we use Hdfs for files with large data sets but not when there are a lot of small files?
    - NameNode stores the metadata information regarding the file systems in RAM.
    - Therefore, the amount of memory produces a limit to the number of files in my HDFS file system. In other words, too many files will lead to the generation of too much metadata.
    - And,storing these metadata in the RAM will become a challenge. Hence, HDFS is only works with large datasets instead of large no.of small.


18. How do you define block, and what is the default block size?
    - Block are nothing but smallest continuous locations in harddrive where data is stored.
    Default block size of hadoop 1 is 64mb and hadoop 2 is 128mb.

19. how do you define Rack awareness in hadoop?
    - Rack Awareness is the algorithm in which the "NameNode" decides how blocks and their replicas are placed, based on rack definitions to minimize network traffic  between "DataNodes" within the same rack.

20. What is difference between hdfs block, and input split?
    - The "HDFS Block" is the physical division of the data while "Input Split" is the logical division of the data. 
    - HDFS divides data into blocks to store the blocks together processing, where input split Divides the data into the input split and assign it to the mapper function for processing.

21. Name of three modes which hadoop can run?
    - Standalone mode
    - Pseudo-distribution mode
    - Fully distributed mode

22. What do you know about SequenceFileFormat?
    - "SequenceFileFormat" is an input format for reading within sequence files.
    - It is a specific compressed binary file format which is optimized for passing the data between the output of one "MapReduce" job to the input of some other "MapReduce" job.

23. What is HIVE?
    - Apache HIVE is a data warehouse system built on top of Hadoop and is used for analyzing structured and semi-structured data developed by Facebook.
    - HIVE abstracts the complexity of Hadoop MapReduce.

24. What is Serde in HIVE?
    - The "Serde" interface allows you to instruct "HIVE" about how a record should be processed.
    - A "SerDe" is a combination of a "Serializer" and a "Deserializer"
    - "Hive" uses "SerDe" (and "FileFormat) to read and write the table's row.

25. Can the default hive metastore used by multiple users at the same time?
    - "Derby database" is the default "Hive Metastore".
    - Mutiple users (processes) cannot access it at the same time.
    - It is mainly used to perform unit tests. 

26. What is the default location for hive to store in table data?
    - The default location where HIVE stores table data is inside HDFS in /user/hive/warehouse


27. What is Apache Hbase?
    - HBase is an open house, multimensional, distributed, scalable, and noSQL database written in JAVA.
    - HBase runs on top of HDFS (Hadoop Distributed File System) and provides Big Table (Google) like capabilities to Hadoop.
    - It is designed to provide a fault-tolerant way of storing the large collection of sparse data sets.
    - HBase achieves high throughput and low latency by providing faster Read/Write Access on huge datasets.

28. What are the components of apache HBase?
    - HBase has three major components 
    ie: HMaster Server, HBase RegionServer and Zookeeper.

    - Region Server: A table can divided into several regions. 
    A group of regions  is served to the clients by a Region Server.

    - HMaster: It coordinates and manages the Region Server( similar as NameNode manages DataNode in HDFS)

    - Zookeeper: Zookeeper acts like as a coordinator inside HBase distributed environment.
    It helps in maintaining server state inside the cluster by communicating through sessions.

29. What are components of Region Server?
    - WAL: Write Ahead log(WAL) is a file attached to every Region Server inside the distributed environment. The WAL stores the new data that hasn't been persisted or committed to permanent storage.
    - Block Cache: Block Cache resides in the top of Region Server. It stores the frequently read data in the memory.
    - MenStore: It is the write cache. It stores all the incoming data before commiting it to the disk or permanent memory. There is one MemStore for each column family in a region.
    - HFile: HFile is stored in HDFS. It stores the actual cells on the disk.

30. WHat is the difference between Hbase and Relation database?
    - HBase is an open source, multimensional, distributed, scalable and a NoSQL database written in JAVA.
    - HBase:                                                            - Relation database
        - It is schema-less                                                  - It is schema-based database
        - It is column-oriented data store                                   - It is row-oriented data store.
        - It is used to store de-normilized data                             - It is used to store normilized data
        - Automated partitioning is done.                                    - There is no such provision or built-in support for partitioning.

31. What is Apache Spark?
    - Apache Spark is a framework for real-time data analytic in ditributed computing environment.
    - It executes in-memory computations to increase the speed of data processing.
    - It is 100x faster than MapReduce for large-scale data processing by exploiting in-memory computations and other optimizations

32. Can you build Spark with any particular Hadoop version?
    - Yes, Spark can be built with any version of Hadoop.

33. What is RDD?
    - RDD is an acronym for Resilient Distribution Datasets - a fault tolerant collection of operational elements that run parallel.
    - The partitioned data in RDD are immutable and ditributed, which is a key component of Apache Spark.

34. Are Hadoop and BigData are co-related?
    - Big Data is an asset, while Hadoop is an open-source software program, which accomplishes a set of goals and objectives to deal with that asset.
    - Hadoop is used to process, store, and analyze complex unstructed data sets through specific proprietary algorithms and methods to derive actionable insights.
    ->  so YES, they are related but are not alike.

35. Why is Hadoop used in bigdata analytic?
    - Hadoop allows running many exploratory data analysis tasks on full datasets, without sampling. 
    - Features that makes Hadoop an essential requirement for BigData are:
            - Data collection
            - Storage 
            - Processing
            - Runs independently

36. Name of some of the important tools used for data analytics?
    - The big data tools are:
        - NodeXL
        - KNIME
        - Tableau
        - Solver
        - OpenRefine
        - Rattle GUI
        - Qlikview

37. What is FDCK?
    - FSCK or File system check is a command used by HDFS.
    - It checks if any file is corrupt, or if there are some missing blocks for a file. FSCK generates a summary report, which lists the overall health of the file system.

38. What are the difference core methods of Reducer?
    - There are three core methods of a reducer:
        - setup() - It helps to configure parameters like heap size, distributed cache, and input data size.
        - reduce() - Also known as once per key with the concerned reduce task. It is the heart of the reducer.
        - cleanup() - It is a process to clean up all the temporary files at the end of a reducer task.

39. What are the most common Input fileformats in Hadoop?
    - The most common input formats in Hadoop are:
        - Key-value input format
        - Sequence file input format
        - Text input format

40. What are the difference fileformats that can be used in Hadoop?
    - File formats used with Hadoop, include - 
        CSV
        JSON
        Columnar
        Sequence files
        AVRO
        Parquet file

41. What is commonity hardware?
    - commodity hardware is the basic hardware resource to run the Apache Hadoop framework.
    - It is a common term used for affordable devices, usually compatible with other such devices.

42. What do you mean by logistic regression?
    - Also known as the logit model, logistic regression is a technique to predict the binary result from a linear amalgamation of predictor variables.

43. Name of porr number for namenode, task tracker, job tracker?
    - NameNode - Port 50070
    - Task Tracker - Port 50060
    - Job Tracker - Port 50030

44. Name the most popular data management tools that used with edge nodes in hadoop?
    - The most commonly used data management tools that work with Edge Nodes in Hadoop are:
        - Oozie 
        - Ambari
        - Pig 
        - Flume 

45. What is block in Hadoop distributed file system?
    - When the file is stored in HDFS, all file system breaks down into a set of blocks.

46. What is the functionality of jps command?
    - The 'jps' command enables us to check if the Hadoop daemons like namenode, datanode, resourcemanager, nodemanager.
    etc: are running on the machine.

47. What types of biases can happen through sampling?
    - Three types of biases can happen through sampling, which are:
        - Survivorship bias
        - Selection bias
        - Under covrage bias

48. What is the difference between Sqoop and Distcp?
    - DistCP is used for transferring data between clusters, while Sqoop is used for transferring data between Hadoop and RDBMS, only.

49. How much data is enough to get a valid outcome?
    - Yes, it is. Hadoop is a distributed file system. It allows us to store and manage large amounts of data in a cloud of machines, managing data redundancy.
    - The main benefit of this is that since the data is stored in multiple nodes, it is better to process it in a distributed way. Each node is ablt to process the data stored on it instead of wasting time moving the data across the network.
    - In contrast, in a relational database computing system, we can query data in real-time, but it is not efficient to store data in tables, records, and columns when the data is huge.
    - Hadoop also provides a schema for building a column database with Hadoop HBase for run-time queries on rows. 

50. What is BackUp Node?
    - A Backup Node is an extended checkpoint node for performing checkpointing and supporting the online streaming of file system edits.
    - Its functionality is similar to checkpoint, and it forces synchronization with NameNode. 


51. What are the common data challenges?
    - The most common data challenges are:
        - Ensuring data integrity 
        - Achieving a 360-degree view
        - Safeguarding user privacy
        - Taking the right business action with real-time resonance.