1. Data Structure Reference:
    https://www.techinterviewhandbook.org/algorithms/study-cheatsheet/
    https://www.interviewcake.com/data-structures-reference
    https://cheatography.com/burcuco/cheat-sheets/data-structures-and-algorithms/pdf/

2. Sorting:
    https://visualgo.net/en/sorting?slide=1-1     - good one with animation 
	https://www.interviewcake.com/sorting-algorithm-cheat-sheet
	https://algs4.cs.princeton.edu/22mergesort/

algorithms: 
    https://www.techinterviewhandbook.org/algorithms/study-cheatsheet/

[1.]Data structures
1.1 Array: 
        arrays hold values of the same type at contiguous memory locations. In an array, we're usually concered about 2 things - the position/index of an element and element itself
        Advantages : accessing elements is fast as long as you have index
        Disadvantages: addition and removal of elements into/from the middle of an array is slow because the remaining elements need to be shifted to accomodate the new/missing elements.
        An exception to this is if the position to be inserted/removed is at the end of the array.
        Time complexity: Access: O(1)
                Insert/search/remove - O(n)

1.2 String: a string is a sequence of characters
        data structures for string: (i) Trie/Prefix (ii) Suffix Tree
        algorithm for string: (i) Rabin Karp (ii) KMP
        time complexity: Access: O(1), insert/search/remove - O(n)

1.3 Stack(abstract datatype)
        stack implements LIFO behavior. 
        ie: set of physical items stacked on top of each other. supports the operations 
        push (insert a new element on the top of the stack)
        and 
        pop (remove and return the most recently added element, the element at the top of the stack)i
        Stacks are an important way of supporting nested or recursive function calls and is used to implement depth-first search.
        Depth-first search can be implemented using recursion or a mannual stack.
        Time complexity: 
        - Top or Peek/push/pop/isempty - O(1), search: O(n)

1.4 Queue(abstract datatype)
        Queue implements FIFO behavior. Similar to people lining up in real life to wait for goods or services. 
        A queue is a linear collection of elements that are maintained in a sequence and can be modified by the addition of elements at one end of the sequence (enqueue operation) and the removal of elements from other end (dequeue operation)
        - end of the sequences at which elements are added is called the back, tail, or rear of the queue.
        - end at which elements are removed is called the head or front of the queue. 
        queues can be implemented using arrays or sligly linked lists. 
        Time complexity: enqueue/dequeue/isEmpty/front/back - O(1)

1.5 Linked List:
        Linked List is used to represent sequential data. It is a linear collection of data elements whose order is not given by their physical placement in memory, as opposed to arrays, where data is stored in sequential blocks of memory.
        Instead, each element contains an address of the next element. It is a data structure containing of a collection of nodes which together represent a sequence.
        In short, each node contains: data, and a reference (in other words, a link) to the next node in the sequence.
        Advantages: insertion/deletion of node is O(1)
        Disadvantages: Access O(n)
        Type of Linked list:
            Singly linked list - each node points to the next node and last node next points to NULL
            Doubly linked list - linked list with two pointers (next and prev). last node's next points to NULL.
            Circular linked list - a singly linked list where last node points back to the first node.
            circular doubly linked list - where the prev pointer of the first node points to the last node and the next pointer of the last node points to the frist node.

1.6 Hash table:
        A data structure that can map keys to values. 
        A hash table uses a hash function on an element to compute an index, also called hash code, into an array of buckets or slots, from which the desired value can be found. 
        During lookup, the key is hashed and the resulting hash indicates where the corresponding values is stored.
        Time complexity: Search / Insert/ Remove - O(1)
        Collection's problems: 
        - Chaining: linked list used to store the collided items 
        - Open addressing: all entry records are in the bucket array itself. When new entry to be inserted, buckets are examined, starting 
        with the hashed-to slot and look to the unoccupied slot is found.

        Addition notes:
        ---------------
        (i): from keys to indices: maping of keys to indices of a hash table called a hash function:
        hash function is the comination of [i] hash-code maps and [ii] compression maps
                    hash code map: key -> integer
                    compression map: integer -> [0, N-1]
        (ii) Popular hash-code maps: 
                (i) integer cast 
                (ii) component sum
                (iii) polymonical accumulation
        (iii) Compression maps: use remainer
        (iv) Choosing hash value size: (i) Use prime number (ensure uniform distribution and prime no, should be close to exact power of 2)
                                        (ii) Given the number of elements (n), choose the reasonable prime number with acceptable no. of collision (eg: 2 to 4)
        (v) Choosing good hash fucntion: (i) quick to compute (ii) distribute keys uniformly thought out the table (iii) minimize the collisions
        (vi) Hashing non-integer keys: convert to ascii values of characters then use standard hash function on the integer
        (vii) Open addressing - (i) linear probing (ii) double Hashing

1.7. Tree
        sets of connected tree. Each node in the tree can be connected to many children, but must be connected to exactly one parent, except for the root node, which has no parent.
        A tree is an undirected and connected acyclic graph. There are no cycles or loops. Each node can be like the root node of its own subtree, making recursion a useful technique of tree traversal.
        Trees are commonly use to represent hierarchical data. eg: file systems, JSON, and HTML documents.

1.7.1 Binary Tree
        Binary means two, so nodes in a binary trees have a maximum of two children.
        Complete binary tree - A complete tree is a binary tree in which every level, except possibly the last, is completely filled, and all nodes in the last level are as far as possible. 
        Balanced binary tree: A binary tree structure in which the left and right subtree of every node differ in height by no more than 1.
Tree traversal:
        In-order traversal - Left -> Root -> right
        Result: 2, 7, 5, 6, 11, 1, 9, 5, 9
        Pre-order traversal - root -> left -> right
        Result: 1, 7, 2, 6, 5, 11, 9, 9, 5
        Post-order traversal - left -> right -> root:
        Result: 2, 5, 11, 6, 7, 5, 9, 9, 1
Binary search tree(BST)
In order traversal of a BST will give you all elements in order
time complexity: access/search/insert/remove - O(log(n))

1.7.2 AVL Tree
        An AVL tree is a type of binary search tree. AVL trees have the property of dynamic self-balancing in addition to all the other properties exhibited by binary search trees.
        Use case: AVL trees are mostly used in in-memory sorts of sets and dictionaries. AVL trees are also used extensively in database applications in which insertions and deletions are fewer but there are frequent lookups for data required.

1.7.3 Red black
        A red-black tree is a kind of self-balancing binary search tree where each node has an extra bit, and that bit is often interpreted as the color (red or black). These colours are used to ensure that the tree remains Balanced during insertions and deletions. 
        Although the balance of the tree is not perfect, it is good enough to reduce the searching time and maintain it around O(log n) time.
        use case: real-world uses of red-black trees includes TreeSet, TreeMap, and Hashmap in the Java collection.

1.8 Graph
        containing a set of objects(nodes or vertices) where three can be edges between these nodes/vertices.
        Edges can be directed or undirected and can optionally have values (a weighted graph)
        Trees can undirected graphs in which any two vertices are connected by exactly one edge and thre can be no cycle in the graph.
        Graph representation:
        (i) Adjacency matrix (ii) Adjacency list (iii) Hash table of hash tables
        Time complexity:
            Depth-first search O(|V| + |E|)
            Breadth-first search O(|V| + |E|)
            Topological sort O(|V| + |E|)

1.9 Heap
        A heap is a specialized tree-based data structure which is a complete tree that satisfies the heap property.
        Max heap - In a max heap the value of a node must be greatest among the node values in its entire subtree. The same property must be recursively true for all nodes in the tree.
        Min heap - In a min heap, the value of a node must be smallest among the node values in its entire subtree. The same property must be recursively true for all nodes in the tree.
        Time complexity:
                Find max/min O(1)
                Insert       O(log(n))
                Remove       O(log(n))
                Heapify (create a heap out of given array of elements)  O(n)

        application: Priority Queue

1.10 Trie 
        Tries are special trees (prefix trees) that make searching and storing strings more efficient. Tries have many pratical applications, such as conducting searches searches and providing autocomplete.
        use case: string auto complete

        Time complexity: Search / Insert/ Remove - O(n)


2. Sorting
        2.1 Insertion sort
            Insertion sort compare the currently selected item with the left side items one by one to find out the location where left side item is <= the current item and right side item is > current item. When the condition match, item will be inserted. SO all the left side items (from the lastly picked item) in the list is sorted and the process will repeated for all the rest of the unsorted items.
            Use case:
            Basically, Insertion sort is efficient for small data values.
            Insertion sort is adaptive in nature. ie: It is appropriate for data sets which are already pratically sorted.
            Time complexity: O(n^2)
        
        2.2 Selection sort
            Selection sort works by repeatedly "selecting" the next-smallest element from the unsorted array and moving it to the front.
            Time complexity: O(n^2)
        
        2.3 Bubble Sort
            works by repeatedly swapping the adjacent elements if they are in the wrong order. This algorithm is not suitable for large data sets as its average and worst-case time complexity is quite high.
            Time complexity: O(n^2)

        2.4 Shell sort
            Shell sort is mainly a variation of insertion sort. In insertion sort, we move elements only one position ahead. When an element has to be moved far ahead, many movements are involved. 
            The idea of Shell sort is to allow the exchange of far items. In shell sort, we make the array h-sorted for a large value of h.
            Time complexity: O(n^2)

        2.5 Merge sort
            Merge sort works by splitting the input in half, recursively sorting each half, and then merging the sorted halves back together.
            ie: The merge sort algorithm is a sorting algorithm that is considered as an example of the divide and conquer strategy. So, in this algorithm, the array is initially divided into two equal halves and they are combined in a sorted manner. We can think of it as a recursive algorithm that continuosly splits the array in half until it cannot be further divided. 
            ie: it is the base case to stop the recursion. If the array has multiple elements, we split the array into halves and recursively ivoke the merge sort on each of the halves. Finally, when both the halves are sorted, the merge operation is applied. Merge operation is the process of taking two smaller sorted arrays and combining them to eventually make a larger one. 
            Time complexity: O(n log n)
        
        2.6 heap sort
            Heapsort is similar to Selection sort - we're repeatedly choosing the largest item and moving it to the end of our array.  But we use a heap to get the largest item more quickly.
            Time complexity: O(n log n)

        2.7 Quick sort
            Quick sort works by recursively dividing the input into smaller arrays around a pivot item: one half has items smaller than the pivot, the other has larger items.
            Quicksort is a Divide and Conquer algorithm. It picks an element as a pivot and partions the given array around the picked pivot.
            We can pick pivot from first, last, middle or random index. Quick sort algorithm will partition the list by putting the pivot element in a current position. 
            ie: all the smaller elements before x and all greater elements after x. All this should be done in linear time.

            quicksort(arr[], low, high) {
                pivotIndex = partition(arr, low, high)
                quicksort(arr, low, pivotIndex-1)
                quicksort(arr, pivotIndex+1, high)
            }

            Time complexity:
                    average case: O(n log n)
                    worst case: O(n^2)

3. searching
    3.1 Linear search
    3.2 Binary search


        
